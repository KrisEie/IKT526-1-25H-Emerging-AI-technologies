{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Introduction to GenAI with Microsoft Fluent UI Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preparing the Microsoft Fluent UI Emojis Dataset\n",
    "\n",
    "In this exercise, we will use the Microsoft Fluent UI Emojis dataset to train a classification model. This dataset contains modern emoji icons in various styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import warnings\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Check for GPU acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare the Microsoft Fluent UI Emojis Dataset\n",
    "\n",
    "We will download the Microsoft Fluent UI Emojis dataset from GitHub and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "dataset_path = os.path.join(os.getcwd(), 'fluent_emojis')\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "print(f\"Dataset will be stored in: {dataset_path}\")\n",
    "\n",
    "# Download Microsoft Fluent UI Emojis dataset\n",
    "def download_fluent_emojis():\n",
    "    # URL to Microsoft Fluent UI Emojis GitHub repository\n",
    "    url = 'https://github.com/microsoft/fluentui-emoji/archive/refs/heads/main.zip'\n",
    "    \n",
    "    print(\"Downloading Microsoft Fluent UI Emojis dataset...\")\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Download complete. Extracting files...\")\n",
    "        \n",
    "        z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "        z.extractall(dataset_path)\n",
    "        print(\"Extraction complete.\")\n",
    "        return os.path.join(dataset_path, 'fluentui-emoji-main')\n",
    "    else:\n",
    "        print(f\"Error downloading: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Download the dataset\n",
    "emoji_path = download_fluent_emojis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "\n",
    "Let's explore the structure of the dataset and look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(emoji_path):\n",
    "    if emoji_path is None or not os.path.exists(emoji_path):\n",
    "        print(\"Dataset is not available.\")\n",
    "        return\n",
    "    \n",
    "    # Find all category folders\n",
    "    categories = [d for d in os.listdir(emoji_path) if os.path.isdir(os.path.join(emoji_path, d)) and not d.startswith('.')]\n",
    "    print(f\"Number of emoji categories: {len(categories)}\")\n",
    "    print(f\"Examples of categories: {categories[:10]}\")\n",
    "    \n",
    "    # Show some examples of emojis\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sample_count = min(5, len(categories))\n",
    "    for i, category in enumerate(categories[:sample_count]):\n",
    "        category_path = os.path.join(emoji_path, category)\n",
    "        style_dirs = [d for d in os.listdir(category_path) if os.path.isdir(os.path.join(category_path, d))]\n",
    "        \n",
    "        for j, style in enumerate(style_dirs[:2]):  # Show 2 styles per category\n",
    "            style_path = os.path.join(category_path, style)\n",
    "            png_files = [f for f in os.listdir(style_path) if f.endswith('.png')]\n",
    "            \n",
    "            if png_files:\n",
    "                img_path = os.path.join(style_path, png_files[0])\n",
    "                img = Image.open(img_path)\n",
    "                plt.subplot(sample_count, 2, i*2 + j + 1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"{category} - {style}\")\n",
    "                plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return categories\n",
    "\n",
    "# Explore the dataset\n",
    "categories = explore_dataset(emoji_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset for Training\n",
    "\n",
    "Now we'll prepare the dataset for training by creating a custom PyTorch Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FluentEmojiDataset(Dataset):\n",
    "    def __init__(self, emoji_path, categories=None, transform=None, max_per_category=50):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        \n",
    "        if categories is None or len(categories) == 0:\n",
    "            return\n",
    "        \n",
    "        # Limit to a manageable number of categories for training\n",
    "        selected_categories = categories[:20]  # Use the first 20 categories\n",
    "        \n",
    "        for idx, category in enumerate(selected_categories):\n",
    "            self.class_to_idx[category] = idx\n",
    "            category_path = os.path.join(emoji_path, category)\n",
    "            style_dirs = [d for d in os.listdir(category_path) if os.path.isdir(os.path.join(category_path, d))]\n",
    "            \n",
    "            count = 0\n",
    "            for style in style_dirs:\n",
    "                style_path = os.path.join(category_path, style)\n",
    "                png_files = [f for f in os.listdir(style_path) if f.endswith('.png')]\n",
    "                \n",
    "                for png_file in png_files:\n",
    "                    if count >= max_per_category:\n",
    "                        break\n",
    "                    img_path = os.path.join(style_path, png_file)\n",
    "                    self.samples.append((img_path, idx))\n",
    "                    count += 1\n",
    "                \n",
    "                if count >= max_per_category:\n",
    "                    break\n",
    "        \n",
    "        print(f\"Total number of images: {len(self.samples)}\")\n",
    "        print(f\"Number of classes: {len(self.class_to_idx)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "dataset = FluentEmojiDataset(emoji_path, categories=categories, transform=transform)\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the CNN Model\n",
    "\n",
    "Now we'll define a simple CNN model to classify the emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmojiCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 8, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "num_classes = len(dataset.class_to_idx)\n",
    "model = EmojiCNN(num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we'll train the model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = val_correct / val_total\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accuracies.append(val_epoch_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} => \"\n",
    "              f\"Training: Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f} | \"\n",
    "              f\"Validation: Loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_acc:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Results\n",
    "\n",
    "Let's visualize the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training')\n",
    "plt.plot(val_accuracies, label='Validation')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model on Some Examples\n",
    "\n",
    "Let's test the model on some examples from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names from indices\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "# Function to show predictions\n",
    "def show_predictions(model, val_loader, idx_to_class, num_samples=5):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(val_loader))\n",
    "    images, labels = images[:num_samples].to(device), labels[:num_samples].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    # Convert images back to display format\n",
    "    images = images.cpu().numpy()\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Denormalization\n",
    "    images = std * images.transpose(0, 2, 3, 1) + mean\n",
    "    images = np.clip(images, 0, 1)\n",
    "    \n",
    "    # Show images with predictions\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        true_label = idx_to_class[labels[i].item()]\n",
    "        pred_label = idx_to_class[preds[i].item()]\n",
    "        title = f\"True: {true_label}\n",
    "Pred: {pred_label}\"\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show some predictions\n",
    "show_predictions(model, val_loader, idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Let's save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = os.path.join(os.getcwd(), 'emoji_classifier_model.pth')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'class_to_idx': dataset.class_to_idx,\n",
    "    'num_classes': num_classes\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we have:\n",
    "1. Downloaded and explored the Microsoft Fluent UI Emojis dataset\n",
    "2. Prepared the dataset for training by creating a custom PyTorch Dataset class\n",
    "3. Defined a CNN model to classify the emojis\n",
    "4. Trained the model on the dataset\n",
    "5. Visualized the training results\n",
    "6. Tested the model on some examples\n",
    "7. Saved the model for later use\n",
    "\n",
    "This is a basic implementation that can be extended in several ways:\n",
    "- Use more advanced CNN architectures like ResNet or EfficientNet\n",
    "- Implement data augmentation to improve model robustness\n",
    "- Use transfer learning from pre-trained models\n",
    "- Experiment with different hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ikt526)",
   "language": "python",
   "name": "ikt526"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
